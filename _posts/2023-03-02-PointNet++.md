---
layout: post
title: "Pointnet++: Deep hierarchical feature learning on point sets in a metric space"
categories: [paper]
---

# Background

LiDAR point clouds are **sparse** and **irregular**. Each point is typically a **4D vector**: 3D coordinates \((x,y,z)\) + reflection **intensity**.  
Because point clouds are unordered sets, **image-style CNNs** (designed for regular grids) struggle to encode features directly.

Earlier works:  
- **Grid-based** (voxel/BEV) methods enable convolutions but suffer from quantization + memory cost.  
- **Point-based** (PointNet) methods operate on raw points but, in the original PointNet, **local geometric structure** wasn’t explicitly modeled.

**PointNet++** addresses this by learning **local→global features hierarchically** over the metric space, improving recognition of **small patterns** and **generalization** on complex scenes.

---

# Paper

## Abstract 

- PointNet++ extends PointNet by **recursively** applying PointNet on **nested partitions** (local neighborhoods) of the point set.  
- It exploits the **distance metric** to build **local feature learners** with **multi-scale, density-adaptive grouping**, boosting robustness to non-uniform sampling.  
- New set-learning layers (**MSG/MRG**) adaptively fuse features across scales for reliable local context.

---

## 1. Introduction

### Learning setup
- **Data:** point sets sampled from a **metric space** (Euclidean in practice).  
- **Invariances:** model must be **permutation-invariant** to point order; the **distance metric** defines meaningful **local neighborhoods**.

However, 3D scanning often yields **non-uniform density** (perspective effects, radial variations, motion). This density variation can hinder stable local feature learning.

### Prior lines
- **PointNet:** per-point spatial encoding via shared MLPs + **global max pooling** → strong global signatures, but **local structure** is not explicitly captured.
- **CNNs on grids:** exploit **multi-resolution hierarchies**; lower layers capture small receptive fields, higher layers capture larger contexts. This hierarchical bias aids generalization.

### Key idea of PointNet++
- Partition the point set into **metric-space neighborhoods**; apply **mini-PointNet** to each local region to learn **local features**; then **group** and **abstract** them hierarchically to form higher-level features.  
- Address two questions: (1) How to create useful partitions? (2) How to learn local abstractions efficiently and robustly?

**Contributions**
1. **Multi-scale neighborhoods** leverage both detail and robustness.  
2. **Random input dropout** during training helps the network **adaptively** combine features from multiple scales under varying densities.  
3. The resulting **hierarchical** network efficiently handles point sets and achieves strong results on 3D tasks.

---

## 2. Problem Statement

Let the input be a metric space:
$$
X = (M, d), \quad M \subset \mathbb{R}^n
$$

- \(X\): the point set with optional per-point attributes (e.g., normals, color).  
- \(d\): the distance metric (Euclidean).  
- Density over \(M\) **need not be uniform**.  
- Goal: learn a set function \(f\) (classification/segmentation) that is **permutation-invariant** and robust to **non-uniform sampling**.

---

## 3. Method

PointNet++ \(=\) **PointNet** + **hierarchical structure**.

### 3.1 Review: PointNet as a set function approximator

Given an unordered set \(\{x_1, \ldots, x_n\}\), PointNet models
$$
f(x_1,\ldots,x_n) \;=\; \gamma\!\left(\max\limits_i\, h(x_i)\right)
$$
where \(h,\gamma\) are shared MLPs and **MAX** is a symmetric aggregator ensuring permutation invariance.

### 3.2 Hierarchical point set feature learning

PointNet++ builds a **stack** of **set abstraction (SA) levels**. Each SA level reduces points and raises abstraction:

- **Sampling layer:** select **centroids** via **Farthest Point Sampling (FPS)** (better coverage than random).  
- **Grouping layer:** form local regions around centroids using either **ball query** (radius-based) or **k-NN**. Ball query enforces a fixed spatial scale.  
- **PointNet layer:** a **mini-PointNet** encodes each local region into a feature vector.

**Input → Output (per SA level)**  
- **Input:** \(N \times (d+C)\) points (coords + features).  
- **Output:** \(N' \times (d+C')\) subsampled points with new local features.  
- Local coordinates are **normalized relative to the centroid** to capture point-to-point relations.

*Illustration*  
![Hierarchical feature learning](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fd60578e-d12c-480d-94c0-ba88c478e22b/Untitled.png)

### 3.3 Robust learning under non-uniform sampling density

Non-uniform density is a major challenge: dense regions allow fine patterns; sparse regions risk missing local structure.

PointNet++ proposes **two density-adaptive strategies**:

![MSG vs MRG](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7daed438-5a8f-44e0-bb45-92aaadb701c9/Untitled.png)

**(a) Multi-Scale Grouping (MSG)**  
- For each centroid, build **multiple radius** groups (small→large).  
- Apply a mini-PointNet **per scale**, then **concatenate** features.  
- Train with **random input dropout** (sample a dropout ratio \(\theta \sim \mathcal{U}(0,p)\), e.g., \(p=0.95\)) so the network learns to **adaptively weight** scales under varying sparsity/uniformity.  
- At test time, use all available points.

**(b) Multi-Resolution Grouping (MRG)**  
- Computationally lighter than MSG.  
- Fuse (1) features aggregated from **lower-level subregions** (higher resolution) and (2) features **directly from raw points** in the local region.  
- When density is low, raw-point features may be more reliable; when dense, subregion summaries capture finer details.  
- Avoids very large-scale neighborhoods at the lowest level → **more efficient** than MSG.

### 3.4 Point feature propagation for segmentation

SA levels **downsample** points; for per-point segmentation we must **up-sample** back:

- Use **interpolation** (inverse distance weighted average with \(k=3, p=2\)) to propagate features from level \(l\) to \(l\!-\!1\).  
- **Skip links** concatenate interpolated features with **saved** features from earlier SA levels.  
- A light “unit PointNet” refines concatenated features.  
- Repeat until features are back on the **original points**.

---

## 4. Experiments

### Datasets
- **2D:** **MNIST** — 60k/10k digits.  
- **3D rigid:** **ModelNet40** — 9,843 train / 2,468 test CAD models across 40 categories.  
- **3D non-rigid:** **SHREC15** — 1,200 shapes from 50 categories (5-fold CV).  
- **Real scenes:** **ScanNet** — 1,201 train / 312 test indoor scenes.

**Point set classification in Euclidean space**

![Random dropout & density adaptation](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4d92e48d-7763-4b3e-9fbc-49a65f1841f6/Untitled.png)  
![Advantage of density-adaptive strategy](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0239adf9-c1de-45a2-8a50-498b5d58dfdb/Untitled.png)

- Left: example point cloud under **random input dropout**.  
- Right: curve illustrating the gain from **density-adaptive** strategy (see §3.3).

**Semantic scene labeling (segmentation)**

![Scene segmentation 1](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c25fc2fa-9b84-4a64-afdc-a2ab354dd6ee/Untitled.png)  
![Scene segmentation 2](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/dad71b2b-b737-45ff-8666-3b60b991591c/Untitled.png)

**Non-Euclidean metric space**

![Non-Euclidean metric experiments](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/aa5fd8cb-7b77-483b-a6b7-8414568bbdbc/Untitled.png)

**Feature visualization**

![Feature visualization](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/19011d83-4499-4e91-98c8-69a32abd6406/Untitled.png)

---

## 5. Related Work
Volumetric CNNs (voxels), multi-view CNNs (2D projections), spectral CNNs (meshes), and PointNet-style set functions.

---

## 6. Conclusion

PointNet++ **integrates local geometry** into PointNet via hierarchical set abstraction.  
**MSG/MRG** make local learning **scale-robust** and **density-adaptive**, improving small-pattern recognition and scene understanding.

The approach is effective but introduces **engineering choices** (radii, sampling counts, grouping strategy, dropout range, etc.)—a reminder that 3D recognition often balances **theory** with **practical design**.

