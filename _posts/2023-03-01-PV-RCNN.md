---
layout: post
title: "PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection"
categories: [paper]
---

# Basic Information
- **Authors:** Shi, S., Guo, C., Jiang, L., Wang, Z., Shi, J., Wang, X., & Li, H
- **Conference:** CVPR 2020, pp. 10529-10538
- **Citations:** 2,684
**Paper Link:** CVF Open Access
  
# Background

![Point Cloud Overview](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9f7a719f-ea51-43a4-8e05-c08ac7787e07/Untitled.png)

Each point in a LiDAR point cloud is a **4D vector** composed of (x, y, z) coordinates and reflection intensity.  
The characteristics of point clouds are:

1. **Sparse**
2. **Irregular**

Because of these properties, conventional CNNs used for image-based object detection are not sufficient for effective feature encoding.  
To address this, many new network architectures have emerged — PV-RCNN being one of them.

---

## 3D Sparse Convolution and Point Set Abstraction

![3D Sparse Convolution](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8c0c2bb9-042e-4143-95cc-19efe6ee995c/Untitled.png)

### 1. 3D Sparse Convolution
Just as 2D convolution is applied to images, 3D convolution can be applied to voxelized data.  
However, 3D convolution is inefficient, so **sparse 3D convolution** is used to process only occupied voxels efficiently.

### 2. Point Set Abstraction
As introduced in **PointNet++**, the point set abstraction encodes local features around sampled keypoints.

- Keypoints are sampled from all points using *Farthest Point Sampling (FPS)*.
- For each keypoint, points within a given radius are grouped.
- Each group is passed through a PointNet block (MLP + Max Pooling) to form a **local feature vector**.

By iteratively applying this, the model learns **hierarchical local features**, effectively encoding the point cloud into fewer representative points.

---

## Point Cloud-Based 3D Object Detection

### 1. Grid-Based Method
Transforms irregular point clouds into **regular representations** such as 3D voxels or 2D BEV maps.  
Then, standard CNN-based detectors are applied.

- Efficient 3D box proposals via 3D sparse convolution.
- **Limitation:** receptive field is limited by convolution kernel size.

### 2. Point-Based Method
Directly uses raw points without voxelization.  
Encodes features through set abstraction operations from PointNet++.

- Provides flexible receptive fields.
- **Limitation:** high computational cost due to pairwise distance calculations.

### 👉 PV-RCNN combines both approaches
- Uses **grid-based region proposals**.
- Refines box locations with **point-based set abstraction**.

---

# 2. PV-RCNN Architecture

## Network Overview

![](https://blog.kakaocdn.net/dn/0lEGR/btqWVSrlz55/y0EgZco5XKGd0ox9zMnxh1/img.png)

PV-RCNN follows a **two-stage detection pipeline**:
1. **Stage 1:** Generate 3D box proposals (grid-based)
2. **Stage 2:** Refine locations and predict confidence scores (point-based)

---

## 3D Voxel CNN for Efficient Feature Encoding

![](https://blog.kakaocdn.net/dn/MEWmv/btqWUMLT0gl/8thSUeKE5H4ZJgG2qLK4xK/img.png)

- Similar to 2D object detection pipelines but with 3D voxelization and sparse convolutions.
- 3×3×3 sparse convolutions downsample the feature volume up to 8×.
- The feature volume is **collapsed along the z-axis** to form a BEV feature map.
- Anchors are defined per pixel for classification and box regression.

---

## Why Combine Grid and Point Methods?

Grid-based methods suffer from:
1. **Resolution loss** due to aggressive downsampling.
2. **Information loss** during upsampling and interpolation.

In contrast, **PointNet-style set abstraction** provides large receptive fields and contextual understanding —  
but with high computational cost.

To mitigate this, PV-RCNN introduces a **two-step set abstraction**:
- First, aggregate features at keypoints.
- Then, use keypoints to compute RoI-grid features.

This reduces pairwise distance computations by ~10–15%.

---

## Voxel-to-Keypoint Scene Encoding via Voxel Set Abstraction

![](https://blog.kakaocdn.net/dn/OzyxQ/btqWVRFYpO0/hk5gCGU06tzeso3KSHDarK/img.png)

1. **Keypoint Sampling:**  
   Sample keypoints via *Farthest Point Sampling* (FPS) uniformly across non-empty voxels.

2. **Feature Aggregation:**  
   For each keypoint, collect voxel features within a radius \( r_k \).  
   Concatenate voxel-wise features \( f \) with relative positions \( v-p \) to form a set \( S \).

3. **Multi-Scale Abstraction:**  
   Different layers use different radii \( r_k \) to achieve flexible receptive fields.

4. **PointNet Block:**  
   Each set passes through an MLP + Max Pooling to produce keypoint feature vectors.

Finally, features from raw points, voxel abstraction, and BEV are concatenated for the final keypoint representation.

---

## Predicted Keypoint Weighting (PKW) Module

![](https://blog.kakaocdn.net/dn/s3c8z/btqWRathjzV/ZYaHgZPAhjyCFCh2nBO8Pk/img.png)

Foreground keypoints contribute more strongly to refinement.

- A segmentation sub-network predicts foreground probability per keypoint.
- Foreground weights are multiplied to keypoint features.
- Foreground labels are derived from whether the point lies inside a ground-truth 3D box.
- **Focal loss** is used to handle class imbalance.

---

## Keypoint-to-Grid RoI Feature Abstraction

![](https://blog.kakaocdn.net/dn/XhgL3/btqWXayfR3A/Vr2NbHJUAkEVHXH1jeQW8K/img.png)

Each 3D proposal contains **6×6×6 grid points (216 total)** sampled uniformly.  
Set abstraction is used again to encode grid features via nearby keypoints.

Unlike average pooling used in earlier methods, this approach captures **richer contextual information**  
and even includes **keypoints outside the RoI boundaries** for boundary-aware refinement.

---

## Proposal Refinement and Confidence Prediction

![](https://blog.kakaocdn.net/dn/bnU8ZJ/btqWX54xMRB/C9v45tBezg9JEkbjqwpfP1/img.png)

- Each RoI’s grid point features are aggregated into a 256D RoI feature vector.
- Two branches:
  - **Confidence Branch:** predicts how likely a proposal matches a ground-truth box (using IoU).
  - **Refinement Branch:** predicts location residuals (center, size, orientation) using **Smooth L1 loss**.

---

## Training Loss

![](https://blog.kakaocdn.net/dn/cwJv1A/btqWUqWF1Yc/PAMQX7KvntZEFXrT3R11V1/img.png)

The total loss combines three terms:

$$
L_{total} = L_{rpn} + L_{seg} + L_{rcnn}
$$

- \( L_{rpn} \): classification + box regression loss for the RPN  
- \( L_{seg} \): segmentation loss for PKW module  
- \( L_{rcnn} \): confidence + box refinement loss

---

# 3. Conclusion

PV-RCNN successfully integrates the **grid-based** and **point-based** paradigms for 3D object detection,  
achieving significant accuracy improvements with reduced computation.

By introducing **keypoints** and **two-step set abstraction**,  
it ensures smooth feature transitions and efficient computation.

However, the method involves numerous hyperparameters — voxel size, feature radii, grid abstraction parameters —  
illustrating how 3D detection tasks are highly **engineering-driven** rather than purely mathematical.

> “At least 15 hyperparameters are tuned in this model —  
> it’s less about elegant equations, and more about clever design.”

---

