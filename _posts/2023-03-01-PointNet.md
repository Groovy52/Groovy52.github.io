---
layout: post
title: "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"
categories: [paper]
---


# Basic Information
- **Authors:** Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas  
- **Conference:** CVPR 2017, pp. 652–660  
- **Citations:** 21,825  
- **Paper Link:** [CVF Open Access](https://openaccess.thecvf.com/content_cvpr_2017/html/Qi_PointNet_Deep_Learning_CVPR_2017_paper.html)  

---

# Background

**1. Rendering**
Rendering (또는 Image synthesis)는 컴퓨터 프로그램을 이용하여 3D 모델이나 장면(scene file)로부터 영상을 만들어내는 과정이다.

**2. Vertex**
Vertex는 꼭짓점 또는 포인트를 의미한다.

**3. Rigid**
Rigid는 변형이 전혀 발생하지 않는, 즉 형태가 고정된 상태를 의미한다.  
Rigid motion은 회전과 평행이동만 허용되는 변환이다.

**4. Understanding Point Cloud Data**
LiDAR 센서나 RGB-D 센서로 수집된 포인트들의 집합.  
각 포인트는 센서에서 반사된 빛의 거리로 계산된 3차원 좌표를 가진다.

<p align="center">
  <img src="/images/papers/rigid_transformation.png" width="250">
  <br><em>Rigid transformation: 형태와 크기를 유지하면서 위치나 방향만 바뀌는 변환. </em>
</p>

<p align="center">
  <img src="/images/papers/affine_transformation.png" width="250">
  <br><em>Affine transformation: 직선, 평행성, 거리 비율을 보존하는 변환. </em>
</p>


### Differences from Image Data

| Property | Image Data | Point Cloud Data |
|-----------|-------------|------------------|
| Coordinate Range | Positive only | Positive and negative |
| Origin | Top-left corner | Sensor position |
| Representation | Integer (pixel index) | Float (x, y, z coordinates) |


### Key Insights
1. **Points are Easy:**  
   Point들은 Scale이나 Rotation을 고려하지 않아도 된다.  
   각 포인트의 위치와 색상 정보만으로 효율적인 데이터 표현이 가능하다.

2. **Various Formats:**  
   `.xyz` (x, y, z만 포함) 또는 `.pcd` (Header + Data 구조) 등 다양한 형식 존재.

3. **PCL (Point Cloud Library):**  
   오픈소스 3D 점군 라이브러리.  
   필터링, 이상치 제거 등 기능 제공.

4. **Data Acquisition Takes Time:**  
   모든 위치를 스캔해야 하므로 데이터 수집 시간이 오래 걸림.

5. **Evolving Field:**  
   향후 점군 데이터의 정확도와 밀도가 빠르게 증가할 것으로 예상됨.

---

# Paper
## Abstract

기존 CNN은 **ordered data**(이미지, 텍스트 등)에 특화되어 있어  
**unordered data**인 point cloud에는 적용이 어렵다.  
기존 시도들은 irregular data를 voxel이나 image로 변환했지만,  
그 결과 데이터 부피 증가 및 정보 손실 문제가 발생했다.  

**PointNet**은 이러한 변환 없이 **raw point cloud**를 직접 입력으로 받아  
강건하게 학습 가능한 네트워크를 제안한다.

---

## 1. Introduction

### Invariance Requirements
1. **Permutation Invariance**  
   입력 순서가 바뀌어도 결과가 동일해야 함.  
   즉, N개의 포인트는 N!개의 조합이 가능하지만 결과는 동일해야 함.

2. **Rigid Motion Invariance**  
   회전이나 평행이동 후에도 classification 결과가 변하지 않아야 함.

### Main Features
- Permutation-invariant architecture for unordered point sets  
- Unified network for classification, segmentation, and scene parsing  
- Simple, effective, and efficient  
- Achieves state-of-the-art performance  
- Empirically and theoretically analyzed for stability and robustness

### Contributions
1. A novel architecture for unordered point sets  
2. Unified framework for multiple 3D tasks  
3. Theoretical and empirical analysis on stability  
4. Visualization and interpretation of learned 3D features

---

## 2. Related Work


### Deep Learning on 3D Data

| Approach            | Representation        | Limitation                        |
|--------------------|-----------------------|-----------------------------------|
| **Volumetric CNN** | 3D voxels             | Sparse, heavy computation         |
| **FPNN, Vote3D**   | Sparse volumes        | Limited scalability               |
| **Multiview CNN**  | 2D projections        | Not extendable to full 3D         |
| **Spectral CNN**   | Mesh manifolds        | Poor for non-isometric shapes     |
| **Feature-based DNN** | Pre-extracted features | Limited representation power   |



### Deep Learning on Unordered Sets
이전 연구는 주로 시퀀스나 NLP에 초점을 맞춰왔고,  
기하학적 unordered 데이터(point cloud)에 대한 연구는 부족했다.

---

## 3. Problem Statement

**Input:** unordered point set (x, y, z)  
**Output:**  
- **Classification:** k class scores  
- **Segmentation:** n×m scores (n = points, m = semantic subcategories)

---

## 4. PointNet Architecture

### 4.1 Properties of Point Sets
1. Unordered  
2. Interaction among points (local geometry)  
3. Invariance under transformations  

---

### 4.2 Architecture Overview

#### 1. Symmetric Function for Unordered Input
- Uses **Max Pooling** as a symmetric function to achieve permutation invariance.  
- MLP layers (64, 64, 64, 128, 1024) applied per point (shared weights).  
- Global feature vector aggregated via max pooling.

$$
f(x_1, \dots, x_n) = \gamma \circ \max(h(x_1), \dots, h(x_n))
$$

#### 2. Local and Global Information Aggregation
- For segmentation, combines per-point local features with global features (concatenation).  
- This allows both local geometry and global context understanding.

#### 3. Joint Alignment Network (T-Net)
- Learns an affine transformation matrix (3×3 for input, 64×64 for feature).  
- Ensures invariance to geometric transformations.  
- Adds regularization to enforce orthogonality of transformation matrices.  

$$
L_{reg} = ||I - AA^T||_F^2
$$

---

### 4.3 Theoretical Analysis
- **Universal Approximation:**  
  Network can approximate any continuous set function given enough capacity.  
- **Stability:**  
  Small input perturbations cause only small output changes (Hausdorff distance continuity).  
- **Bottleneck Effect:**  
  Global max pooling acts as a stable feature bottleneck.

---

## 5. Experiments

### 5.1 Applications
1. **3D Object Classification (ModelNet40)**  
   - 12,311 samples (9,843 train / 2,468 test)  
   - 1024 points per model, normalized to unit sphere  
   - Data augmentation: rotation + Gaussian noise  
2. **Object Part Segmentation**  
3. **Scene Semantic Segmentation**

### Results
- Achieved state-of-the-art accuracy and robustness  
- Visualized per-point segmentation with high fidelity  

---

## 6. Qualitative Results
<p align="center">
  <img src="/images/pointnet_segmentation.png" width="500">
  <br><em>Semantic segmentation examples from the PointNet paper.</em>
</p>

---

## Key Takeaways
- Simple yet powerful architecture for unordered 3D point clouds  
- Max pooling ensures permutation invariance  
- T-Net guarantees geometric invariance  
- Foundation for later works (PointNet++, DGCNN, Point Transformer)

