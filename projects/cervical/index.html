<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unreadable Image Classification | Goeun Kim </title> <meta name="author" content="Goeun Kim"> <meta name="description" content="A Radiomics-based Unread Cervical Imaging Classification Algorithm"> <meta name="keywords" content="computer vision, 3d perception, lidar, deep learning, autonomous robotics"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?v=935bb6a8b0884b3a161fdd1791bd0500"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="/projects/cervical/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Goeun Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unreadable Image Classification</h1> <p class="post-description">A Radiomics-based Unread Cervical Imaging Classification Algorithm</p> </header> <article> <h2 id="1-overview">1. Overview</h2> <p>This study proposes a <strong>radiomics-based pre-screening framework</strong> to automatically identify and exclude unreadable medical images prior to deep learning model training. Unlike conventional research that focuses on improving network architectures, this work demonstrates that <strong>input data quality is a primary determinant of AI performance.</strong></p> <p>By filtering diagnostically unreliable images before training, the framework reduces label noise and stabilizes feature learning, ultimately improving model accuracy and reliability in clinical environments.</p> <hr> <h2 id="2-motivation">2. Motivation</h2> <p>Real-world clinical datasets inevitably contain images degraded by:</p> <ul> <li>Motion artifacts</li> <li>Low contrast or acquisition errors</li> <li>Partial anatomical capture</li> <li>Severe noise or distortion</li> </ul> <p>Such samples are often <strong>difficult even for clinicians to interpret</strong>, yet they remain in training datasets and introduce:</p> <ul> <li>Implicit label noise</li> <li>Feature ambiguity</li> <li>Unstable decision boundaries</li> </ul> <p>Rather than increasing model complexity, this study explores a data-centric question: <strong>“Can AI performance improve simply by learning from diagnostically reliable data?”</strong></p> <p>This perspective aligns with the shift toward <strong>Data-Centric AI</strong>, where dataset design becomes as critical as model architecture.</p> <hr> <h2 id="3-proposed-approach">3. Proposed Approach</h2> <p>Fig. 2 illustrates the overall workflow of the proposed method for automatically classifying unreadable cervical images. The algorithm evaluates whether an image is diagnostically usable by quantifying brightness, blur, and anatomical positioning. All metric values are normalized between <strong>0 (0%) and 1 (100%)</strong> using the full distribution of readable-image data.</p> <h4 id="3-1-classification-indicators">3-1. Classification Indicators</h4> <p>Subtle morphological variations in cervical epithelial cells significantly influence diagnostic interpretation. Therefore, pixel-level image quality differences can directly affect readability. As shown in <strong>Fig. 2</strong>, three criteria are defined to classify unreadable images:</p> <p><strong>1) Brightness</strong></p> <p>The first indicator is the <strong>average brightness value</strong> of the image.</p> <ul> <li>Pixel intensities are summed and divided by the total pixel count to compute the mean brightness.</li> <li>Histograms are used to simultaneously visualize readable vs. unreadable datasets and compare their distributions.</li> <li>The acceptable brightness range is determined statistically using <strong>t-test analysis and histogram-based thresholding.</strong> </li> <li>Images with extreme brightness deviations that hinder visual interpretation are classified as unreadable.</li> </ul> <p><strong>2) Blur</strong></p> <p>The second indicator is the <strong>degree of blur</strong>, defined as the sharpness of the image.</p> <ul> <li>Blur is measured using the <strong>Laplacian operator</strong>, a second-order derivative widely used for edge detection.</li> <li>Lower values indicate blurred images; higher values indicate clearer images.</li> <li>Based on statistical analysis of readable images, the acceptable threshold is set within the <strong>densely distributed upper 13% range of blur values.</strong> </li> <li>This relative definition reflects the dataset-dependent nature of blur measurement.</li> </ul> <p><strong>3) Region Adequacy (Cervical Position)</strong></p> <p>The third indicator evaluates whether the cervical os region is properly captured.</p> <ul> <li>A template image with the cervical region centered is defined.</li> <li>Similarity between the template and an input image is computed using <strong>Euclidean distance.</strong> </li> </ul> <p>Examples of unreadable cases identified using these criteria are presented in Fig. 3, including:</p> <ul> <li>excessively bright or dark images,</li> <li>blurred images,</li> <li>images where the cervical region is improperly captured.</li> </ul> <h4 id="3-2-classification-procedure">3-2. Classification Procedure</h4> <p>The full decision process is depicted in <strong>Fig. 4</strong> and proceeds as follows:</p> <p><strong>1) Input Stage</strong> A cervical image is provided to the algorithm.</p> <p><strong>2) Threshold-Based Evaluation</strong> The image is tested against predefined thresholds for brightness, blur, and Euclidean-distance-based region adequacy. Any image outside these ranges is classified as unreadable and excluded from model input.</p> <p><strong>3) Cause-Oriented Categorization</strong> Each of the three indicators has a binary outcome (acceptable / unacceptable), producing:</p> <p>2×2×2=8 unreadable-case combinations</p> <p>The algorithm assigns each rejected image to one of these eight categories and outputs a message describing the specific cause. Images satisfying all criteria are retained and recommended for expert diagnosis.</p> <p><strong>Normalized Indicator Interpretation</strong></p> <p>Within the normalized classification standard:</p> <ul> <li>Brightness: Values closer to 1 indicate higher intensity.</li> <li>Blur: Larger values indicate clearer images.</li> <li>Region (Euclidean Distance): Smaller values indicate better alignment of the cervical region.</li> </ul> <hr> <h2 id="4-implementation-details">4. Implementation Details</h2> <h4 id="4-1-data-acquisition">4-1. Data Acquisition</h4> <p>A total of 2,257 cervical images were collected using a cervical imaging diagnostic device (Dr. Cervicam, NTL Healthcare).</p> <ul> <li>The study protocol was approved by the Institutional Review Board (IRB No. EUMC 2015-10-004), and all data were fully anonymized prior to analysis.</li> <li>To ensure clear clinical separability, the dataset was composed using extreme stages of the cervical cancer progression spectrum, following the WHO classification system.</li> </ul> <p>According to the WHO framework, cervical cancer develops through the following stages:</p> <p><strong>Normal → Atypical → LSIL → HSIL → Invasive Cervical Cancer</strong></p> <p>For this study:</p> <ul> <li> <strong>Readable Images (2,000 total)</strong> <ul> <li>1,000 Normal cases (control group)</li> <li>1,000 HSIL cases (lesion group)</li> </ul> </li> <li> <strong>Unreadable Images (257 total)</strong> <ul> <li>Images judged by experts as diagnostically unusable.</li> </ul> </li> </ul> <p>All images were stored as <strong>24-bit JPG files</strong> and resized to <strong>256 × 256 pixels</strong> before model processing.</p> <h4 id="4-2-data-preprocessing-and-augmentation-process">4-2. Data Preprocessing and Augmentation Process</h4> <p>Unreadable images in the collected dataset were initially labeled by clinical experts due to issues such as abnormal brightness, reflections, or other visual defects that prevented cervical identification.</p> <p><strong>Resolution and Aspect Ratio Normalization</strong> The raw dataset contained heterogeneous image sizes (2048×1536, 1280×960, 1504×1000) and varying aspect ratios.</p> <p>To standardize spatial representation:</p> <ul> <li>The shorter side of each image was used as the reference dimension.</li> <li>Images were center-cropped to produce a consistent square format.</li> </ul> <p><strong>Removal of Non-Anatomical Tags</strong></p> <p>Some images contained metadata tags in the lower-right corner. These tags could be mistakenly interpreted as anatomical structures by AI models.</p> <p>Therefore:</p> <ul> <li>Tag regions were removed prior to analysis to prevent false feature learning.</li> </ul> <p><strong>Center Alignment of the Cervical Region</strong></p> <p>The diagnostically important cervical os was not always located at the image center.</p> <ul> <li>The main anatomical region was spatially normalized,</li> <li>Feature calculations reflected anatomical characteristics rather than positional bias.</li> </ul> <h4 id="4-2-software">4-2. Software</h4> <p><strong>Framework</strong></p> <ul> <li>The proposed network and all comparative models were implemented using TensorFlow/Keras, which was the most widely used framework for transfer learning with ResNet50 in medical imaging research.</li> <li>Standalone Keras (v2.4) was already being phased into tf.keras, making TensorFlow-integred Keras the more likely setup.</li> </ul> <p><strong>Training</strong></p> <ul> <li>Optimization was performed using the Adam optimizer.</li> <li>Batch size: 16</li> <li>Training epochs: 300</li> <li>A cosine annealing learning rate schedule was adopted: <ul> <li>Initial learning rate: 0.0001</li> </ul> </li> </ul> <p><strong>Hardware</strong></p> <ul> <li>The environment utilized CUDA 10.1</li> </ul> <hr> <h2 id="5-results">5. Results</h2> <hr> <h2 id="6-technical-takeaways">6. Technical Takeaways</h2> <p>This work demonstrates that:</p> <h4 id="6-1-data-centric-ai-can-rival-model-centric-improvements">6-1. Data-Centric AI Can Rival Model-Centric Improvements</h4> <p>Performance gains were achieved without changing the backbone network, emphasizing that dataset reliability is a first-order factor in AI success.</p> <h4 id="6-2-image-readability-can-be-quantified">6-2. Image Readability Can Be Quantified</h4> <p>Radiomics enables translation of subjective clinical judgments into:</p> <ul> <li>Reproducible numerical metrics</li> <li>Automated dataset validation</li> <li>Scalable quality control pipelines</li> </ul> <h4 id="6-3-practical-deployment-value">6-3. Practical Deployment Value</h4> <p>This framework is immediately applicable to:</p> <ul> <li>Pre-training dataset validation</li> <li>Multi-center data harmonization</li> <li>Clinical AI safety pipelines</li> </ul> <hr> <h2 id="7-future-work">7. Future Work</h2> <h4 id="7-1-quality-aware-training-soft-weighting-instead-of-hard-filtering">7-1. Quality-Aware Training (Soft Weighting Instead of Hard Filtering)</h4> <p>Incorporate readability scores directly into loss weighting.</p> <h4 id="7-2-artifact-localization-models">7-2. Artifact Localization Models</h4> <p>Move from detecting whether an image is degraded to where degradation occurs.</p> <h4 id="7-3-automated-data-curation-pipelines">7-3. Automated Data-Curation Pipelines</h4> <p>Continuous dataset refinement during model lifecycle (active data governance).</p> <h4 id="7-4-multi-modality-extension">7-4. Multi-Modality Extension</h4> <p>Apply the framework to CT, MRI, ultrasound, and CBCT environments.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Goeun Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>