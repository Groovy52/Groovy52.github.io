<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unreadable Image Classification | Goeun Kim </title> <meta name="author" content="Goeun Kim"> <meta name="description" content="A Radiomics-based Unread Cervical Imaging Classification Algorithm"> <meta name="keywords" content="computer vision, 3d perception, lidar, deep learning, autonomous robotics"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?v=935bb6a8b0884b3a161fdd1791bd0500"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="/projects/cervical/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Goeun Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unreadable Image Classification</h1> <p class="post-description">A Radiomics-based Unread Cervical Imaging Classification Algorithm</p> </header> <article> <h2 id="1-overview">1. Overview</h2> <p>This study proposes a <strong>radiomics-based pre-screening framework</strong> to automatically identify and exclude unreadable medical images prior to deep learning model training. Unlike conventional research that focuses on improving network architectures, this work demonstrates that <strong>input data quality is a primary determinant of AI performance.</strong></p> <p>By filtering diagnostically unreliable images before training, the framework reduces label noise and stabilizes feature learning, ultimately improving model accuracy and reliability in clinical environments.</p> <hr> <h2 id="2-motivation">2. Motivation</h2> <p>Real-world clinical datasets inevitably contain images degraded by:</p> <ul> <li>Motion artifacts</li> <li>Low contrast or acquisition errors</li> <li>Partial anatomical capture</li> <li>Severe noise or distortion</li> </ul> <p>Such samples are often <strong>difficult even for clinicians to interpret</strong>, yet they remain in training datasets and introduce:</p> <ul> <li>Implicit label noise</li> <li>Feature ambiguity</li> <li>Unstable decision boundaries</li> </ul> <p>Rather than increasing model complexity, this study explores a data-centric question: <strong>“Can AI performance improve simply by learning from diagnostically reliable data?”</strong></p> <p>This perspective aligns with the shift toward <strong>Data-Centric AI</strong>, where dataset design becomes as critical as model architecture.</p> <hr> <h2 id="3-proposed-approach">3. Proposed Approach</h2> <p>Figure 2 illustrates the overall workflow of the proposed method for automatically classifying unreadable cervical images. The algorithm evaluates whether an image is diagnostically usable by quantifying brightness, blur, and anatomical positioning. All metric values are normalized between <strong>0 (0%) and 1 (100%)</strong> using the full distribution of readable-image data.</p> <h4 id="3-1-classification-indicators">3-1. Classification Indicators</h4> <p>Subtle morphological variations in cervical epithelial cells significantly influence diagnostic interpretation. Therefore, pixel-level image quality differences can directly affect readability. As shown in <strong>Fig. 2</strong>, three criteria are defined to classify unreadable images:</p> <p><strong>1) Brightness</strong></p> <p>The first indicator is the <strong>average brightness value</strong> of the image.</p> <ul> <li>Pixel intensities are summed and divided by the total pixel count to compute the mean brightness.</li> <li>Histograms are used to simultaneously visualize readable vs. unreadable datasets and compare their distributions.</li> <li>The acceptable brightness range is determined statistically using <strong>t-test analysis and histogram-based thresholding.</strong> </li> <li>Images with extreme brightness deviations that hinder visual interpretation are classified as unreadable.</li> </ul> <p><strong>2) Blur</strong></p> <p>The second indicator is the <strong>degree of blur</strong>, defined as the sharpness of the image.</p> <ul> <li>Blur is measured using the <strong>Laplacian operator</strong>, a second-order derivative widely used for edge detection.</li> <li>Lower values indicate blurred images; higher values indicate clearer images.</li> <li>Based on statistical analysis of readable images, the acceptable threshold is set within the <strong>densely distributed upper 13% range of blur values.</strong> </li> <li>This relative definition reflects the dataset-dependent nature of blur measurement.</li> </ul> <p><strong>3) Region Adequacy (Cervical Position)</strong></p> <p>The third indicator evaluates whether the cervical os region is properly captured.</p> <ul> <li>A template image with the cervical region centered is defined.</li> <li>Similarity between the template and an input image is computed using <strong>Euclidean distance</strong> </li> </ul> <p>Examples of unreadable cases identified using these criteria are presented in Figure 3, including:</p> <ul> <li>excessively bright or dark images,</li> <li>blurred images,</li> <li>images where the cervical region is improperly captured.</li> </ul> <h4 id="3-2-classification-procedure">3-2. Classification Procedure</h4> <p>The full decision process is depicted in <strong>Figure 4</strong> and proceeds as follows:</p> <p><strong>1) Input Stage</strong> A cervical image is provided to the algorithm.</p> <p><strong>2) Threshold-Based Evaluation</strong> The image is tested against predefined thresholds for brightness, blur, and Euclidean-distance-based region adequacy. Any image outside these ranges is classified as unreadable and excluded from model input.</p> <p><strong>3) Cause-Oriented Categorization</strong> Each of the three indicators has a binary outcome (acceptable / unacceptable), producing:</p> <p>2×2×2=8 unreadable-case combinations</p> <p>The algorithm assigns each rejected image to one of these eight categories and outputs a message describing the specific cause. Images satisfying all criteria are retained and recommended for expert diagnosis.</p> <p><strong>Normalized Indicator Interpretation</strong></p> <p>Within the normalized classification standard:</p> <ul> <li>Brightness: Values closer to 1 indicate higher intensity.</li> <li>Blur: Larger values indicate clearer images.</li> <li>Region (Euclidean Distance): Smaller values indicate better alignment of the cervical region.</li> </ul> <hr> <h2 id="4-implementation-details">4. Implementation Details</h2> <h4 id="4-1-data-acquisition">4-1. Data Acquisition</h4> <p>The dataset was collected specifically for this study. Due to hospital privacy regulations, it cannot be publicly released. Access may be granted upon institutional agreement.</p> <p><strong>LiDAR Sensor Configuration</strong></p> <p>A total of 10,985 point cloud scenes were collected at the Veterans Health Service Medical Center between 2022 and 2023 using flash-type LiDAR sensors (NSL-1110AV, NANOSYSTEMS Corp., Gyeongsan, South Korea) operating at 5 frames per second (FPS).</p> <p>LiDAR systems are broadly categorized into scanning and flash types. Unlike scanning LiDAR, which sequentially emits laser beams, flash LiDAR captures the entire scene simultaneously, providing robust and stable sensing for fixed indoor installations with simpler hardware and improved durability.</p> <p>Although flash LiDAR offers lower spatial resolution and a narrower field of view than scanning LiDAR, it is well suited for hospital environments, where reliable operation and privacy-preserving depth sensing are more important than long-range perception.</p> <p>Each captured point cloud had a spatial resolution of 320 × 240 voxels with a sensing depth of up to 12 m.</p> <p><strong>Sensor Deployment in a Hospital Environment</strong></p> <p>A total of 19 LiDAR sensors were installed in fixed positions across four representative hospital zones: Radiology Department, Laboratory Medicine, Inpatient Ward A, and Inpatient Ward B</p> <p>These locations were deliberately selected to ensure:</p> <ul> <li>spatial diversity of indoor geometries</li> <li>varied human–robot interaction scenarios</li> <li>realistic collision-risk environments for AMR deployment.</li> </ul> <p>This multi-zone configuration enabled the dataset to capture heterogeneous clinical workflows and dynamic object interactions, producing a representative benchmark for hospital navigation systems.</p> <h4 id="4-2-data-preprocessing-and-augmentation-process">4-2. Data Preprocessing and Augmentation Process</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sald-net_fig5-480.webp 480w,/assets/img/sald-net_fig5-800.webp 800w,/assets/img/sald-net_fig5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sald-net_fig5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig 5. Preprocessing steps for raw point cloud data. (a) Raw point cloud. (b) Voxel-based downsampling. (c) RANSAC-based filtering; red points indicate filtered wall points. (d) Final denoised point cloud after statistical outlier removal. Background points are shown in black; object points and bounding boxes are color-coded by object class </div> <p>To stabilize noisy hospital point clouds, a four-stage pipeline was designed:</p> <ul> <li> <strong>Voxel-based downsampling for density normalization Voxel-based downsampling was applied with a voxel size of 0.25 m:</strong> <ul> <li>projects points into voxel grids,</li> <li>replaces points within each voxel by their centroid,</li> <li>reduces point density while maintaining global geometry.</li> </ul> </li> <li> <strong>RANSAC filtering to remove structural planes To separate foreground objects from structural background, a RANSAC-based plane fitting algorithm was applied with:</strong> <ul> <li>distance threshold: 0.2 m</li> <li>minimum sampled points: 3</li> <li>maximum iterations: 500</li> </ul> </li> <li> <strong>Statistical outlier removal for sensor noise reduction To suppress measurement noise, statistical filtering was performed using:</strong> <ul> <li>number of neighbors: 20</li> <li>standard deviation ratio: 1.5</li> </ul> </li> </ul> <p><strong>Data Augmentation</strong> The preprocessed dataset of 10,985 scenes is split into 6,591 training, 2,197 validation, and 2,197 test samples. To mitigate class imbalance among robots, people, beds, and wheelchairs, GT sampling is adopted, a widely used data augmentation method originally introduced in SECOND.</p> <p>Specifically, annotated objects from a database are inserted into other training scenes. This augmentation increases the number of robots, beds, and wheelchairs, as summarized in Table 1.</p> <p>To avoid object overlaps, the z-coordinate of each inserted object is set to the lowest height among existing objects. For horizontal positioning, inserted objects are placed beyond the largest existing x or y coordinates in the scene, depending on the spatial distribution of existing objects.</p> <p>If existing objects are located along the positive x-axis, new objects are placed further along the y-axis, and vice versa. Finally, scenes are normalized by centering the 3D coordinate system at (0, 0, 0), ensuring consistent spatial scaling across the dataset.</p> <h4 id="4-3-software">4-3. Software</h4> <p><strong>Framework</strong></p> <ul> <li>The proposed network and all comparative models were implemented using PyTorch 1.9.1.</li> <li>Baseline methods in Table 2 were reproduced using the OpenPCDet toolbox, a widely adopted open-source framework for 3D object detection.</li> <li>Except for adapting configurations to the hospital-specific dataset, all models retained their default settings provided in the official repository to ensure fair comparison.</li> </ul> <p><strong>Training</strong></p> <ul> <li>Optimization was performed using the Adam optimizer.</li> <li>Batch size: 4</li> <li>Training epochs: 100</li> <li>A cosine annealing learning rate schedule was adopted: <ul> <li>Initial learning rate: 0.001</li> <li>Increased gradually to a peak of 0.01 during the first 40% of training steps</li> <li>Decreased smoothly to a minimal value over the remaining 60% of training.</li> </ul> </li> <li>Online augmentation techniques included: <ul> <li>Random flipping</li> <li>Random rotation in the range of −45° to 45°</li> <li>Scaling with factors between 0.95 and 1.05</li> </ul> </li> </ul> <p><strong>Hardware</strong></p> <ul> <li>Training was conducted on an NVIDIA GeForce RTX 3090 GPU.</li> <li>The environment utilized CUDA 11.1 for GPU acceleration.</li> </ul> <hr> <h2 id="5-results">5. Results</h2> <p>SALD-Net significantly outperformed the baseline Part-A2 detector:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sald-net_fig6-480.webp 480w,/assets/img/sald-net_fig6-800.webp 800w,/assets/img/sald-net_fig6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sald-net_fig6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Qualitative comparison of 3D object detection results from baseline methods and SALD-Net on the test set. Background points are shown in black, and object points are color-coded by class. Top down BEV images provide overall scene context, while zoomed-in 3D RoIs highlight mispredicted objects. Detection errors—misclassified, missed, or over-detected—are indicated by arrows. Ground-truth and predicted objects are shown in red and aqua-blue bounding boxes, respectively </div> <ul> <li><strong>3D mAP: 89.08%</strong></li> <li>Overall improvement: <strong>+19.56%</strong> </li> <li>Wheelchair detection: +22.85%p The model successfully separated objects that previous detectors failed to distinguish in cluttered hospital scenes</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sald-net_Table2-480.webp 480w,/assets/img/sald-net_Table2-800.webp 800w,/assets/img/sald-net_Table2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sald-net_Table2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table 2. Qualitative Performance comparison of 3D detection on our test dataset. The evaluation metrics are BEV AP(%), 3D AP(%) with an IoU threshold of 0.5 for robot, person, bed, and wheelchair classes, and inference speed measured in FPS </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sald-net_Table3-480.webp 480w,/assets/img/sald-net_Table3-800.webp 800w,/assets/img/sald-net_Table3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sald-net_Table3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table 3. Ablation study results on the test set. Evaluation of the impact of data augmentation and self-attention mechanisms in different net work modules. AUG: Applying data augmentation for the training set, BAM: backbone-integrated self-attention mechanism, RAM: RoI feature-based self-attention mechanism </div> <hr> <h2 id="6-technical-takeaways">6. Technical Takeaways</h2> <p>This work demonstrates that:</p> <ul> <li>Indoor medical environments require <strong>domain-specific perception modeling</strong> </li> <li>Global relational reasoning is critical for dense human-object interaction scenes</li> <li>Dataset realism is as important as model architecture for safety-critical robotics</li> </ul> <hr> <h2 id="7-future-work">7. Future Work</h2> <p>Future extensions include:</p> <ul> <li>Multi-sensor fusion for improved spatial robustness</li> <li>Deployment-oriented optimization for real-time AMR navigation</li> <li>Transfer of the preprocessing pipeline to radar-based perception systems</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Goeun Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>