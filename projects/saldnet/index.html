<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SALD-Net | Goeun Kim </title> <meta name="author" content="Goeun Kim"> <meta name="description" content="Self-attention-integrated LiDAR-based 3D object detection network in a crowded hospital environment"> <meta name="keywords" content="computer vision, 3d perception, lidar, deep learning, autonomous robotics"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?v=935bb6a8b0884b3a161fdd1791bd0500"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://groovy52.github.io/projects/saldnet/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Goeun Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">SALD-Net</h1> <p class="post-description">Self-attention-integrated LiDAR-based 3D object detection network in a crowded hospital environment</p> </header> <article> <h2 id="1-overview">1. Overview</h2> <p>Autonomous mobile robots (AMRs) are increasingly deployed in hospitals to mitigate workforce shortages and support healthcare delivery by performing tasks such as medication transport and patient guidance.</p> <p>However, accurate <strong>3D object detection in hospital environments</strong> is significantly more difficult than in outdoor autonomous driving scenarios due to dense human-object interactions, privacy constraints, and sensor limitations.</p> <p>This project proposes <strong>SALD-Net</strong>, a self-attention-based 3D detection framework designed specifically for hospital LiDAR environments.</p> <hr> <h2 id="2-why-this-problem-matters">2. Why This Problem Matters</h2> <h3 id="2-1-why-hospital-detection-is-challenging">2-1. Why Hospital Detection Is Challenging</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sald-net_fig1-480.webp 480w,/assets/img/sald-net_fig1-800.webp 800w,/assets/img/sald-net_fig1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sald-net_fig1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of challenges in 3D object detection in hospital environments. Background points are shown in black; object points and bounding boxes are color-coded by object class. (a) Occlusion: A person is partially occluded by a bed. (b) Overlap: Adjacent objects exhibit overlapping regions. (c) Combined: Both occlusion and overlap occur simultaneously. (d) Sparsity: Non-uniform sensor density leads to incomplete 3D representations </div> <p>Hospital environments exhibit unique domain characteristics:</p> <ul> <li>Frequent overlapping interactions between patients, staff, beds, and wheelchairs</li> <li>Narrow and cluttered corridors causing severe occlusion and partial observations</li> <li>Lack of domain-specific datasets compared to public benchmark datasets such as KITTI and Waymo which focus on outdoor driving scenes beds and wheelchairs</li> <li>Strict privacy regulations preventing RGB-based perception → depth-only sensing required</li> </ul> <p>As a result, models trained on datasets such as KITTI or Waymo suffer from strong domain shift</p> <hr> <h2 id="3-challenges">3. Challenges</h2> <p>The project addressed four key problems:</p> <h3 id="3-1-domain-gap">3-1. Domain Gap</h3> <p>The lack of domain-specific datasets and the presence of occluded or specialized objects (e.g., beds, wheelchairs) pose significant challenges</p> <h3 id="3-2-sensor-noise--low-resolution">3-2. Sensor Noise &amp; Low Resolution</h3> <p>Flash LiDAR introduced outliers and unclear object boundaries</p> <h3 id="3-3-class-imbalance">3-3. Class Imbalance</h3> <p>Wheelchairs and beds appeared far less frequently than people</p> <h3 id="3-4-occlusion--overlapping-objects">3-4. Occlusion &amp; Overlapping Objects</h3> <p>Dense multi-object motion made instance separation extremely difficult</p> <hr> <h2 id="4-method">4. Method</h2> <h3 id="41-self-attention-based-detection-architecture">4.1 Self-Attention-Based Detection Architecture</h3> <p>SALD-Net introduces two self-attention modules:</p> <ul> <li> <p><strong>BAM (Backbone-integrated self-attention mechanism)</strong><br> Enhances global geometric dependency beyond PointNet++ local features.</p> </li> <li> <p><strong>Hierarchical grid RoI pooling &amp; RAM (RoI feature-based self-attention mechanism)</strong><br> Refines proposals using contextual relationships between neighboring objects.</p> </li> </ul> <p>This enables robust detection under occlusion and overlapping scenarios</p> <p>+ These coefficients update each embedding as follows: $v_e’ = v_e + v_{\text{coef}} \odot p_e$, which incorporates spatial information into the value embedding; $p_e’ = q_{\text{coef}} \odot p_e$, which adjusts the position embedding; $k_e’ = k_{\text{coef}} \odot k_e$, which refines the semantic key embedding; and $pk_e’ = qk_{\text{coef}} \odot pk_e$, which models their interaction. The attention map is computed as $\text{Attention map} = p_e’ + k_e’ + pk_e’$, which is then normalized via softmax and used to weight the updated value embedding $v_e’$ across $N$ neighboring points. The final aggregated feature is obtained as: \begin{equation} —</p> <h2 id="5-implementation-details">5. Implementation Details</h2> <h3 id="5-1-data-acquisition">5-1. Data Acquisition</h3> <p><strong>LiDAR Sensor Configuration</strong> A total of 10,985 point cloud scenes were collected at the Veterans Health Service Medical Center between 2022 and 2023 using flash-type LiDAR sensors (NSL-1110AV, NANOSYSTEMS Corp., Gyeongsan, South Korea) operating at 5 frames per second (FPS).</p> <p>LiDAR systems are broadly categorized into scanning and flash types. Unlike scanning LiDAR, which sequentially emits laser beams, flash LiDAR captures the entire scene simultaneously, providing robust and stable sensing for fixed indoor installations with simpler hardware and improved durability.</p> <p>Although flash LiDAR offers lower spatial resolution and a narrower field of view than scanning LiDAR, it is well suited for hospital environments, where reliable operation and privacy-preserving depth sensing are more important than long-range perception.</p> <p>Each captured point cloud had a spatial resolution of 320 × 240 voxels with a sensing depth of up to 12 m.</p> <p><strong>Sensor Deployment in a Hospital Environment</strong> A total of 19 LiDAR sensors were installed in fixed positions across four representative hospital zones: Radiology Department, Laboratory Medicine, Inpatient Ward A, and Inpatient Ward B</p> <p>These locations were deliberately selected to ensure:</p> <ul> <li>spatial diversity of indoor geometries</li> <li>varied human–robot interaction scenarios</li> <li>realistic collision-risk environments for AMR deployment.</li> </ul> <p>This multi-zone configuration enabled the dataset to capture heterogeneous clinical workflows and dynamic object interactions, producing a representative benchmark for hospital navigation systems.</p> <h3 id="5-2-data-preprocessing-and-augmentation-process">5-2. Data Preprocessing and Augmentation Process</h3> <p><strong>Preprocessing Pipeline</strong></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sald-net_fig5-480.webp 480w,/assets/img/sald-net_fig5-800.webp 800w,/assets/img/sald-net_fig5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sald-net_fig5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Preprocessing steps for raw point cloud data. (a) Raw point cloud. (b) Voxel-based downsampling. (c) RANSAC-based filtering; red points indicate filtered wall points. (d) Final denoised point cloud after statistical outlier removal. Background points are shown in black; object points and bounding boxes are color-coded by object class </div> <p>To stabilize noisy hospital point clouds, a four-stage pipeline was designed:</p> <ul> <li>Voxel-based downsampling for density normalization Voxel-based downsampling was applied with a voxel size of 0.25 m: <ul> <li>projects points into voxel grids,</li> <li>replaces points within each voxel by their centroid,</li> <li>reduces point density while maintaining global geometry.</li> </ul> </li> <li>RANSAC filtering to remove structural planes To separate foreground objects from structural background, a RANSAC-based plane fitting algorithm was applied with: <ul> <li>distance threshold: 0.2 m</li> <li>minimum sampled points: 3</li> <li>maximum iterations: 500</li> </ul> </li> <li>Statistical outlier removal for sensor noise reduction To suppress measurement noise, statistical filtering was performed using: <ul> <li>number of neighbors: 20</li> <li>standard deviation ratio: 1.5</li> </ul> </li> </ul> <p><strong>Data Augmentation</strong> The preprocessed dataset of 10,985 scenes is split into 6,591 training, 2,197 validation, and 2,197 test samples. To mitigate class imbalance among robots, people, beds, and wheelchairs, we adopt GT sampling, a widely used data augmentation method originally introduced in SECOND.</p> <p>Specifically, annotated objects from a database are inserted into other training scenes. This augmentation increases the number of robots, beds, and wheelchairs, as summarized in Table 1.</p> <p>To avoid object overlaps, the z-coordinate of each inserted object is set to the lowest height among existing objects. For horizontal positioning, inserted objects are placed beyond the largest existing x or y coordinates in the scene, depending on the spatial distribution of existing objects.</p> <p>If existing objects are located along the positive x-axis, new objects are placed further along the y-axis, and vice versa. Finally, scenes are normalized by centering the 3D coordinate system at (0, 0, 0), ensuring consistent spatial scaling across the dataset [21].</p> <h3 id="5-3-software">5-3. Software</h3> <p><strong>Framework</strong></p> <ul> <li>The proposed network and all comparative models were implemented using PyTorch 1.9.1.</li> <li>Baseline methods in Table 2 were reproduced using the OpenPCDet toolbox, a widely adopted open-source framework for 3D object detection.</li> <li>Except for adapting configurations to the hospital-specific dataset, all models retained their default settings provided in the official repository to ensure fair comparison.</li> </ul> <p><strong>Training</strong></p> <ul> <li>Optimization was performed using the Adam optimizer.</li> <li>Batch size: 4</li> <li>Training epochs: 100</li> <li>A cosine annealing learning rate schedule was adopted: <ul> <li>Initial learning rate: 0.001</li> <li>Increased gradually to a peak of 0.01 during the first 40% of training steps</li> <li>Decreased smoothly to a minimal value over the remaining 60% of training.</li> </ul> </li> <li>Online augmentation techniques included: <ul> <li>Random flipping</li> <li>Random rotation in the range of −45° to 45°</li> <li>Scaling with factors between 0.95 and 1.05</li> </ul> </li> </ul> <p><strong>Hardware</strong></p> <ul> <li>Training was conducted on an NVIDIA GeForce RTX 3090 GPU.</li> <li>The environment utilized CUDA 11.1 for GPU acceleration.</li> </ul> <hr> <h2 id="6-results">6. Results</h2> <p>SALD-Net significantly outperformed the baseline Part-A2 detector:</p> <ul> <li><strong>3D mAP: 89.08%</strong></li> <li>Overall improvement: <strong>+19.56%</strong> </li> <li>Wheelchair detection: +22.85%p The model successfully separated objects that previous detectors failed to distinguish in cluttered hospital scenes</li> </ul> <hr> <h2 id="7-technical-takeaways">7. Technical Takeaways</h2> <p>This work demonstrates that:</p> <ul> <li>Indoor medical environments require <strong>domain-specific perception modeling</strong> </li> <li>Global relational reasoning is critical for dense human-object interaction scenes</li> <li>Dataset realism is as important as model architecture for safety-critical robotics</li> </ul> <hr> <h2 id="8-future-work">8. Future Work</h2> <p>Future extensions include:</p> <ul> <li>Multi-sensor fusion for improved spatial robustness</li> <li>Deployment-oriented optimization for real-time AMR navigation</li> <li>Transfer of the preprocessing pipeline to radar-based perception systems</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Goeun Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>