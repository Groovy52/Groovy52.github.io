<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SALD-Net | Goeun Kim </title> <meta name="author" content="Goeun Kim"> <meta name="description" content="Self-attention-integrated LiDAR-based 3D object detection network in a crowded hospital environment"> <meta name="keywords" content="computer vision, 3d perception, lidar, deep learning, autonomous robotics"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?v=935bb6a8b0884b3a161fdd1791bd0500"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="/projects/saldnet/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Goeun Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">SALD-Net</h1> <p class="post-description">Self-attention-integrated LiDAR-based 3D object detection network in a crowded hospital environment</p> </header> <article> <h2 id="1-overview">1. Overview</h2> <p><strong>Autonomous mobile robots (AMRs) are increasingly deployed in hospitals</strong> to mitigate workforce shortages through supporting tasks such as medication transport and patient guidance.</p> <p>However, <strong>accurate 3D object detection in hospital environments is significantly more challenging</strong> than in outdoor autonomous driving scenarios due to dense human–object interactions, privacy constraints, and sensor limitations.</p> <p>In this project, <strong>SALD-Net, a self-attention integrated LiDAR-based 3D object detection network in a crowded hospital environment</strong> is proposed.</p> <hr> <h2 id="2-motivation">2. Motivation</h2> <h4 id="2-1-why-hospital-detection-is-challenging">2-1. Why Hospital Detection Is Challenging</h4> <div class="row justify-content-center"> <div class="col-sm-8 col-md-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sald-net_fig1-480.webp 480w,/assets/img/sald-net_fig1-800.webp 800w,/assets/img/sald-net_fig1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sald-net_fig1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> **Fig 1.** Illustration of challenges in 3D object detection in hospital environments. B**ackground points are shown in black; object points and bounding boxes are color-coded by object class. (a) Occlusion: A person is partially occluded by a bed. (b) Overlap: Adjacent objects exhibit overlapping regions. (c) Combined: Both occlusion and overlap occur simultaneously. (d) Sparsity: Non-uniform sensor density leads to incomplete 3D representations </div> <p><strong>Four key challenges due to unique domain characteristics of hospitals:</strong></p> <p><strong>1) Domain Gap</strong> The lack of domain-specific datasets and the presence of occluded or specialized objects (e.g., beds and wheelchairs) introduce significant detection difficulty.</p> <p><strong>2) Sensor Noise &amp; Low Resolution</strong> Flash LiDAR data are characterized by outliers and indistinct object boundaries.</p> <p><strong>3) Class Imbalance</strong> Wheelchairs and beds occur far less frequently than people, resulting in skewed training distributions.</p> <p><strong>4) Occlusion &amp; Overlapping Objects</strong> Dense multi-object motion due to move assistance makes instance separation highly challenging.</p> <hr> <h2 id="3-proposed-approach">3. Proposed Approach</h2> <h4 id="31-self-attention-based-detection-architecture">3.1 Self-Attention-Based Detection Architecture</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sald-net_fig2-480.webp 480w,/assets/img/sald-net_fig2-800.webp 800w,/assets/img/sald-net_fig2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sald-net_fig2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig 2. The architecture of SALD-Net. The backbone extracts features via backbone-integrated self-attention mechanism (BAM). Foreground segmentation generates initial 3D box proposals, which are refined by the unified regional and grid (URG) RoI pooling head, enhanced with RoI feature-based self-attention mechanism (RAM). Fully connected (FC) layers output final confidence scores and bounding boxes </div> <p>SALD-Net is designed as a <strong>two-stage end-to-end 3D object detector</strong> tailored for <strong>flash-LiDAR indoor environments</strong>, where point clouds are sparse, low-resolution, and frequently occluded.</p> <ul> <li> <strong>Stage 1</strong>: Initial 3D proposal generation via PointNet++ backbone</li> <li> <strong>Stage 2</strong>: Proposal refinement using a Unified Regional and Grid (URG) RoI pooling head</li> </ul> <p><strong>3-1-1. Backbone-integrated self-attention mechanism (BAM)</strong></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sald-net_fig3-480.webp 480w,/assets/img/sald-net_fig3-800.webp 800w,/assets/img/sald-net_fig3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sald-net_fig3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig 2. Architecture details of the backbone-integrated self-attention mechanism (BAM) </div> <p>BAM models long-range relationships between point groups while preserving continuous geometry.</p> <p>Given grouped features $X = {x_1, \dots, x_n}$, global dependencies are captured by connecting all node pairs through a self-attention formulation applied prior to geometric discretization. Each node feature $x_i$ represents a local point-wise grouped feature and $n$ is the number of groups $(i = 1, \dots, n)$. To capture global dependencies, all node pairs $(x_i, x_j)$ are connected via an edge set $E = {r_{ij}, i,j = 1, \dots, n}$, where each edge $r_{ij}$ represents the interaction between the $i^{\text{th}}$ and $j^{\text{th}}$ nodes. These interaction terms are computed using a self-attention mechanism.</p> <p>In BAM, each node feature $x_j$ is projected via linear layers into a key vector $K_j$ and a value vector $V_j$, while a query vector $Q_i$ is derived from $x_i$. The attention weight $W_{ij}$ between the $i^{\text{th}}$ and $j^{\text{th}}$ point nodes is computed as $W_{ij} = \text{softmax}(Q_i \cdot K_j^\top)$. The interaction term $r_{ij}$ is obtained by multiplying the $W_{ij}$ and the $V_j$ as $r_{ij} = W_{ij} \cdot V_j$. A global context-aware feature $a_i = \sum_{j=1}^{n} r_{ij}$ is obtained and aggregated via multi-head attention, then concatenated with the local node feature $x_i$ and upsampled through feature propagation. To address class imbalance between foreground and background regions, focal loss is applied. The architecture of BAM is illustrated in Fig. 3.</p> <p>This allows each local region to adaptively gather global structural cues without converting points into tokens or voxels:</p> <ul> <li>Geometry-aware context modeling</li> <li>Lightweight compared to full transformers</li> </ul> <p><strong>3-1-2. Unified Regional and Grid (URG) RoI Pooling head</strong></p> <p>Indoor medical objects (beds, wheelchairs, AMRs):</p> <ul> <li>vary greatly in scale,</li> <li>are partially occluded,</li> <li>contain limited LiDAR returns.</li> </ul> <p>URG combines:</p> <ul> <li> <strong>Regional RoI pooling</strong> → captures surrounding context</li> <li> <strong>Hierarchical grid pooling</strong> → extracts multi-scale internal structure</li> </ul> <p>This hybrid pooling preserves both external context and internal geometry for robust detection.</p> <p><strong>3-1-3. RoI feature-based self-attention mechanism (RAM)</strong></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sald-net_fig4-480.webp 480w,/assets/img/sald-net_fig4-800.webp 800w,/assets/img/sald-net_fig4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sald-net_fig4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Architecture details of the RoI feature-based self-attention mechanism (RAM) </div> <p>The RoI feature-based self-attention mechanism (RAM) refines the grouped point set from the hierarchical grid RoI pooling by integrating spatial and semantic relationships.</p> <p>The architecture of RAM is illustrated in Figure~\ref{fig4}. The RAM takes as input the 3D coordinates $F_{\text{xyz}} \in \mathbb{R}^{N \times 3}$ and semantic features $F_f \in \mathbb{R}^{N \times C}$ of $N$ neighboring points. From $F_{\text{xyz}}$, a position embedding $p_e$ is computed via a fully connected (FC) layer. Semantic inputs $F_f$ are projected to key and value embeddings, $k_e$ via an FC layer, and $v_e$ via an MLP.</p> <p>These coefficients update each embedding as follows: $v_e’ = v_e + v_{\text{coef}} \odot p_e$, which incorporates spatial information into the value embedding; $p_e’ = q_{\text{coef}} \odot p_e$, which adjusts the position embedding; $k_e’ = k_{\text{coef}} \odot k_e$, which refines the semantic key embedding; and $pk_e’ = qk_{\text{coef}} \odot pk_e$, which models their interaction. The attention map is computed as $\text{Attention map} = p_e’ + k_e’ + pk_e’$, which is then normalized via softmax and used to weight the updated value embedding $v_e’$ across $N$ neighboring points. The final aggregated feature is obtained as: \begin{equation} F_{\text{agg}} = \sum_{i=1}^{N} \text{Softmax}(\text{Attention map}^{(i)}) \odot v_e’^{(i)} \label{eq1} \end{equation}</p> <hr> <h2 id="4-implementation-details">4. Implementation Details</h2> <h4 id="4-1-data-acquisition">4-1. Data Acquisition</h4> <p>The dataset was collected specifically for this study. Due to hospital privacy regulations, it cannot be publicly released. Access may be granted upon institutional agreement.</p> <p><strong>LiDAR Sensor Configuration</strong></p> <p>A total of 10,985 point cloud scenes were collected at the Veterans Health Service Medical Center between 2022 and 2023 using flash-type LiDAR sensors (NSL-1110AV, NANOSYSTEMS Corp., Gyeongsan, South Korea) operating at 5 frames per second (FPS).</p> <p>LiDAR systems are broadly categorized into scanning and flash types. Unlike scanning LiDAR, which sequentially emits laser beams, flash LiDAR captures the entire scene simultaneously, providing robust and stable sensing for fixed indoor installations with simpler hardware and improved durability.</p> <p>Although flash LiDAR offers lower spatial resolution and a narrower field of view than scanning LiDAR, it is well suited for hospital environments, where reliable operation and privacy-preserving depth sensing are more important than long-range perception.</p> <p>Each captured point cloud had a spatial resolution of 320 × 240 voxels with a sensing depth of up to 12 m.</p> <p><strong>Sensor Deployment in a Hospital Environment</strong></p> <p>A total of 19 LiDAR sensors were installed in fixed positions across four representative hospital zones: Radiology Department, Laboratory Medicine, Inpatient Ward A, and Inpatient Ward B</p> <p>These locations were deliberately selected to ensure:</p> <ul> <li>spatial diversity of indoor geometries</li> <li>varied human–robot interaction scenarios</li> <li>realistic collision-risk environments for AMR deployment.</li> </ul> <p>This multi-zone configuration enabled the dataset to capture heterogeneous clinical workflows and dynamic object interactions, producing a representative benchmark for hospital navigation systems.</p> <h4 id="4-2-data-preprocessing-and-augmentation-process">4-2. Data Preprocessing and Augmentation Process</h4> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sald-net_fig5-480.webp 480w,/assets/img/sald-net_fig5-800.webp 800w,/assets/img/sald-net_fig5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sald-net_fig5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig 5. Preprocessing steps for raw point cloud data. (a) Raw point cloud. (b) Voxel-based downsampling. (c) RANSAC-based filtering; red points indicate filtered wall points. (d) Final denoised point cloud after statistical outlier removal. Background points are shown in black; object points and bounding boxes are color-coded by object class </div> <p>To stabilize noisy hospital point clouds, a four-stage pipeline was designed:</p> <ul> <li> <strong>Voxel-based downsampling for density normalization Voxel-based downsampling was applied with a voxel size of 0.25 m:</strong> <ul> <li>projects points into voxel grids,</li> <li>replaces points within each voxel by their centroid,</li> <li>reduces point density while maintaining global geometry.</li> </ul> </li> <li> <strong>RANSAC filtering to remove structural planes To separate foreground objects from structural background, a RANSAC-based plane fitting algorithm was applied with:</strong> <ul> <li>distance threshold: 0.2 m</li> <li>minimum sampled points: 3</li> <li>maximum iterations: 500</li> </ul> </li> <li> <strong>Statistical outlier removal for sensor noise reduction To suppress measurement noise, statistical filtering was performed using:</strong> <ul> <li>number of neighbors: 20</li> <li>standard deviation ratio: 1.5</li> </ul> </li> </ul> <p><strong>Data Augmentation</strong> The preprocessed dataset of 10,985 scenes is split into 6,591 training, 2,197 validation, and 2,197 test samples. To mitigate class imbalance among robots, people, beds, and wheelchairs, GT sampling is adopted, a widely used data augmentation method originally introduced in SECOND.</p> <p>Specifically, annotated objects from a database are inserted into other training scenes. This augmentation increases the number of robots, beds, and wheelchairs, as summarized in Table 1.</p> <p>To avoid object overlaps, the z-coordinate of each inserted object is set to the lowest height among existing objects. For horizontal positioning, inserted objects are placed beyond the largest existing x or y coordinates in the scene, depending on the spatial distribution of existing objects.</p> <p>If existing objects are located along the positive x-axis, new objects are placed further along the y-axis, and vice versa. Finally, scenes are normalized by centering the 3D coordinate system at (0, 0, 0), ensuring consistent spatial scaling across the dataset.</p> <h4 id="4-3-software">4-3. Software</h4> <p><strong>Framework</strong></p> <ul> <li>The proposed network and all comparative models were implemented using PyTorch 1.9.1.</li> <li>Baseline methods in Table 2 were reproduced using the OpenPCDet toolbox, a widely adopted open-source framework for 3D object detection.</li> <li>Except for adapting configurations to the hospital-specific dataset, all models retained their default settings provided in the official repository to ensure fair comparison.</li> </ul> <p><strong>Training</strong></p> <ul> <li>Optimization was performed using the Adam optimizer.</li> <li>Batch size: 4</li> <li>Training epochs: 100</li> <li>A cosine annealing learning rate schedule was adopted: <ul> <li>Initial learning rate: 0.001</li> <li>Increased gradually to a peak of 0.01 during the first 40% of training steps</li> <li>Decreased smoothly to a minimal value over the remaining 60% of training.</li> </ul> </li> <li>Online augmentation techniques included: <ul> <li>Random flipping</li> <li>Random rotation in the range of −45° to 45°</li> <li>Scaling with factors between 0.95 and 1.05</li> </ul> </li> </ul> <p><strong>Hardware</strong></p> <ul> <li>Training was conducted on an NVIDIA GeForce RTX 3090 GPU.</li> <li>The environment utilized CUDA 11.1 for GPU acceleration.</li> </ul> <hr> <h2 id="5-results">5. Results</h2> <p>SALD-Net significantly outperformed the baseline Part-A2 detector:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sald-net_fig6-480.webp 480w,/assets/img/sald-net_fig6-800.webp 800w,/assets/img/sald-net_fig6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sald-net_fig6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Qualitative comparison of 3D object detection results from baseline methods and SALD-Net on the test set. Background points are shown in black, and object points are color-coded by class. Top down BEV images provide overall scene context, while zoomed-in 3D RoIs highlight mispredicted objects. Detection errors—misclassified, missed, or over-detected—are indicated by arrows. Ground-truth and predicted objects are shown in red and aqua-blue bounding boxes, respectively </div> <ul> <li><strong>3D mAP: 89.08%</strong></li> <li>Overall improvement: <strong>+19.56%</strong> </li> <li>Wheelchair detection: +22.85%p The model successfully separated objects that previous detectors failed to distinguish in cluttered hospital scenes</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sald-net_Table2-480.webp 480w,/assets/img/sald-net_Table2-800.webp 800w,/assets/img/sald-net_Table2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sald-net_Table2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table 2. Qualitative Performance comparison of 3D detection on our test dataset. The evaluation metrics are BEV AP(%), 3D AP(%) with an IoU threshold of 0.5 for robot, person, bed, and wheelchair classes, and inference speed measured in FPS </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sald-net_Table3-480.webp 480w,/assets/img/sald-net_Table3-800.webp 800w,/assets/img/sald-net_Table3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sald-net_Table3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table 3. Ablation study results on the test set. Evaluation of the impact of data augmentation and self-attention mechanisms in different net work modules. AUG: Applying data augmentation for the training set, BAM: backbone-integrated self-attention mechanism, RAM: RoI feature-based self-attention mechanism </div> <hr> <h2 id="6-technical-takeaways">6. Technical Takeaways</h2> <p>This work demonstrates that:</p> <ul> <li>Indoor medical environments require <strong>domain-specific perception modeling</strong> </li> <li>Global relational reasoning is critical for dense human-object interaction scenes</li> <li>Dataset realism is as important as model architecture for safety-critical robotics</li> </ul> <hr> <h2 id="7-future-work">7. Future Work</h2> <p>Future extensions include:</p> <ul> <li>Multi-sensor fusion for improved spatial robustness</li> <li>Deployment-oriented optimization for real-time AMR navigation</li> <li>Transfer of the preprocessing pipeline to radar-based perception systems</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Goeun Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>